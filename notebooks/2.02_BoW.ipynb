{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "<img src=\"../media/img/kisz_logo.png\" width=\"192\" height=\"69\"> \n",
    "<h1> NLP Fundamentals\n",
    "<h2> Working with Embeddings\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f6a800; color: #ffffff; padding: 10px;\">\n",
    "<h2>Part 2.2 - Bag of Words\n",
    "</div>\n",
    "\n",
    "In this section we will build our first vector representation based on a statistical model, Bag of Words, from scratch and using a special package called <kbd>gensim</kbd>. We will then make our first query and check how this vector representation works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nb_config\n",
    "\n",
    "from src.data import data_loader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>1. Overview\n",
    "</div>\n",
    "\n",
    "The Bag of Words (BoW) model is a simplified representation used in Natural Language Processing and Text Analysis. It treats a document as an unordered set of words, disregarding grammar and word order but focusing on word frequency.\n",
    "\n",
    "Here's a brief explanation of how the Bag of Words model works:\n",
    "- Tokenization:\n",
    "    The first step involves breaking down a document into tokens. We have already done that!\n",
    "\n",
    "- Building the Vocabulary:\n",
    "    A unique vocabulary is created by compiling a list of all distinct tokens present in the entire set of documents. Each token in this vocabulary is assigned a unique index or identifier.\n",
    "\n",
    "- Vectorization:\n",
    "    For each document in the dataset, a numerical vector is constructed. The length of the vector is equal to the size of the vocabulary, and each position corresponds to the count or presence of a specific word in the document. If a word is present, its corresponding position is marked; otherwise, it is set to zero.\n",
    "\n",
    "- Document Comparison:\n",
    "    The Bag of Words model allows for the comparison of documents based on their word vectors. Similar documents will have similar vector representations, despite variations in word order or grammar. We will compare documents with the metrics we discuss before.\n",
    "\n",
    "Our first step will be loading the data we have tokenized in our previous part, and the arguments passed to the <kbd>normalize()</kbd> function, when we created the tokens. This info will be used later, when we tokenize the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, params = data_loader(\"my_tokenized_data.parquet\")\n",
    "\n",
    "# extract the arguments for the normalize function\n",
    "tokenizer = params['tokenizer']\n",
    "arguments = params['args']\n",
    "\n",
    "# print the tokenizer used getting the tokens\n",
    "print(f\"Tokenizer used: {tokenizer}\")\n",
    "print(f\"Arguments passed to the normalize function:\\n{arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>2. Creating the vocabulary\n",
    "</div>\n",
    "\n",
    "As we said before, the vocabulary refers to the complete set of unique words present in a given set of documents or corpus. It represents the entirety of distinct words used across documents and serves as the guide for building afterwards our vector representations.\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Create a python list that contains all the unique tokens in our descriptors and store it in a variable called <kbd>vocabulary</kbd>.\n",
    "</div>\n",
    "\n",
    "\n",
    "The vocabulary can't be stored in a set. It needs to be a list, or even better, a tuple. That choice is primarily related to the need to maintain order and mapping between words and their corresponding indices.\n",
    "\n",
    "We will create a variable called *tokens* for easier access to the column *'tokens'* in our dataframe.\n",
    "\n",
    ">\n",
    "> <details>\n",
    "> <summary>Need help? Here some possible solutions</summary>\n",
    "> \n",
    "> 1. Using a for loop over all the elemens in the column:\n",
    "> <pre><code># create an empty set\n",
    "> vocabulary = set()\n",
    "> \n",
    "> # iterate over the pd Series and updates the set\n",
    "> for index in df.index:\n",
    ">    vocabulary.update(df.loc[index, 'tokens'].tolist())\n",
    "> \n",
    "> # turn the vocabulary into a list\n",
    "> vocabulary = list(vocabulary)\n",
    "> \n",
    "> </code></pre>\n",
    "> \n",
    "> <hr align=left width=350>\n",
    "> \n",
    "> 2. Using <kbd>chain</kbd> from the package <kbd>itertools</kbd>:\n",
    "> <pre><code>from itertools import chain\n",
    "> \n",
    "> # chain all token lists, convert the result into a set\n",
    "> # and then turn it into a list\n",
    "> vocabulary = list(set(chain(*tokens)))\n",
    "> \n",
    "> </code></pre>\n",
    "> \n",
    "> <hr align=left width=350>\n",
    "> \n",
    "> 3. Using <kbd>corpora</kbd> from the package <kbd>gensim</kbd>:\n",
    "> <pre><code>from gensim import corpora\n",
    "> \n",
    "> # create a Dictionary object from the tokens\n",
    "> dictionary = corpora.Dictionary(tokens)\n",
    "> \n",
    "> # filter the tokens with a set and turn it into a list\n",
    "> vocabulary = list({value for key, value in dictionary.items()})\n",
    "> \n",
    "> </code></pre>\n",
    "> This last system has the advantage of allowing us to make really cool things based on the structure of the texts, e.g. filtering directly tokens that appear in too many or too few texts. In addition, it has an interesting collection of other useful methods.\n",
    "> </details>\n",
    ">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an easy access to the tokens\n",
    "tokens = df['tokens']\n",
    "\n",
    "# your code here\n",
    "...\n",
    "...\n",
    "vocabulary = ...\n",
    "\n",
    "# print vocabulary size\n",
    "print(f\"The vocabulary has {len(vocabulary)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to the first 15 tokens in our vocabulary, before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*vocabulary[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>3. Vectorizing the documents\n",
    "</div>\n",
    "\n",
    "We are going to construct now a numerical vector For each document in the dataset. The length of the vector is equal to the size of the vocabulary, and each position corresponds to the count or presence of a specific word in the document. This results in a numerical representation of the document based on the words it contains.\n",
    "\n",
    "We have here to different possibilities:\n",
    "- **binary BoW**, where we track with a 1 or a 0 the presence of the token in the document\n",
    "- **Term Frequency BoW**, where we track not only the presence but we also count the number of ocurrences of the token in the document and use that number as the vector value for that token in that document.\n",
    "\n",
    "We will start with the binary Bag of Words, but you should be able later to reuse the same code with small modifications for trying the TF-BoW.\n",
    "\n",
    "<br>\n",
    "\n",
    "> <details>\n",
    "> <summary>About sparse representations...</summary>\n",
    "> As most documents contain only a small subset of the entire vocabulary, the resulting vectors are typically sparse, meaning that the majority of entries are zero. In those cases we will try to code the vectors as sparse representations.\n",
    "> This sparse representations help manage computational resources efficiently by reducing the amount of memory and processing power required: only the non-zero elements need to be stored and processed, resulting in significant savings in terms of both storage space and computational effort.\n",
    "> </details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Create a sparse representation of the documents using the **binary Bag of Words** model. Follow the next steps:\n",
    "1. Create a *scipy* sparse matrix called <kbd>bow_sparse</kbd> that has as many rows as documents in our dataset and as many columns as tokens in our vocabulary. The datatype should integer.\n",
    "2. Index the tokens in the vocabulary to their position in the vocabulary with a dictionary called <kbd>string_to_index</kbd>.\n",
    "3. Populate the matrix with nested for loops.\n",
    "</div>\n",
    "\n",
    "For pedagogical reasons, we will also create a pandas DataFrame with the matrix and use the tokens in our vocabulary as column names.\n",
    "\n",
    "\n",
    "> <details>\n",
    "> <summary>You don't know what to do?</summary>\n",
    "> \n",
    "> Maybe this code can help you:\n",
    "> \n",
    "> <pre><code># create the sparse matrix\n",
    "> bow_sparse = dok_matrix((len(tokens), len(vocabulary)), dtype=np.int32)\n",
    "> \n",
    "> # dictionary with pairs 'token':index\n",
    "> string_to_index = {string: i for i, string in enumerate(vocabulary)}\n",
    "> \n",
    "> # populate the matrix\n",
    "> for i, string_list in enumerate(tokens):\n",
    ">    for string in string_list:\n",
    ">        bow_sparse[i, string_to_index[string]] = 1\n",
    "> </code></pre>\n",
    "> </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use dok_matrix from scipy\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "# create the sparse matrix\n",
    "bow_sparse = ...\n",
    "\n",
    "# dictionary with pairs 'token':index\n",
    "string_to_index = {...}\n",
    "\n",
    "# populate the matrix\n",
    "for ...:\n",
    "    for ...:\n",
    "        ...\n",
    "\n",
    "# Convert the DOK matrix to a sparse DataFrame\n",
    "bow_df = pd.DataFrame.sparse.from_spmatrix(bow_sparse, columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that dataframe look like? Well, really sparse..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>4. Making our first query\n",
    "</div>\n",
    "\n",
    "Time to look for a film. Let's say we are interested in watching again an old film where one guy (was it Tim Robbins?) is sent to prison but he is innocent, and even though the conditions are really harsh he doesn't give up and ends up doing the warden a favour with his accountant skills? We don't remember the name of the film. So let's see if our model can find it for us.\n",
    "\n",
    "We are going to write a text describing briefly the movie. Something like:\n",
    "\n",
    "> *An innocent man goes to prison accused of killing his wife and her lover, but never loses the hope*\n",
    "\n",
    "We will pass that sentence through a tokenizer. We will use the metadata we collected before and create a tokenizer from scratch with that metadata, so we will have exactly the same tokenizer we used for the rest of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"An innocent man in prison that never loses the hope starts helping the warden as accountant\"\n",
    "\n",
    "print(tokenizer)\n",
    "print(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Create a vector representation of the query. We will follow this steps:\n",
    "1. Tokenize the query the same way we did with the texts in our dataset\n",
    "2. Make a list with all the unique tokens in the query and store in the variable <kbd>query_tokens</kbd>\n",
    "3. Create a scipy sparse matrix as we did for the corpus vectors with just one row and as many columns as tokens in our vocabulary. Store it in the variable <kbd>query_sparse</kbd>\n",
    "4. Populate the sparse matrix with nested for loops, as we did before\n",
    "</div>\n",
    "\n",
    "Here again, we will create a pandas DataFrame to store the query vector.\n",
    "\n",
    "> <details>\n",
    "> <summary>We are sure you don't need the solution, but just in case...</summary>\n",
    "> \n",
    "> There you go. This code should work:\n",
    "> \n",
    "> <pre><code>from src.normalizing import normalize, NLTKTokenizer  # <- import the tokenizer you need\n",
    ">  \n",
    "> # instantiate the tokenizer\n",
    "> tkn = NLTKTokenizer()\n",
    "> \n",
    "> # tokenize the query with the same tokenizer you used for the corpus texts\n",
    "> query_tokens = list(set(normalize(text=query, tkn=tkn, **arguments)[1]))\n",
    "> \n",
    "> # create the sparse matrix\n",
    "> query_sparse = dok_matrix((1, len(vocabulary)), dtype=np.int32)\n",
    "> \n",
    "> # populate the matrix\n",
    "> for string in query_tokens:\n",
    ">     if string in vocabulary:\n",
    ">         query_sparse[0, string_to_index[string]] = 1\n",
    "> </code></pre>\n",
    "> </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.normalizing import normalize, ... # <- import the tokenizer you need\n",
    "\n",
    "# instantiate the tokenizer\n",
    "tkn = ...\n",
    "\n",
    "# tokenize the query with the same tokenizer you used for the corpus texts\n",
    "query_tokens = ...\n",
    "\n",
    "# create the sparse matrix\n",
    "query_sparse = ...\n",
    "\n",
    "# populate the matrix\n",
    "for ...:\n",
    "    if ...:\n",
    "        ...\n",
    "\n",
    "# Convert the DOK matrix to a sparse DataFrame\n",
    "query_df = pd.DataFrame.sparse.from_spmatrix(query_sparse, columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to compare our vectors. We will take advantage of NumPy's optimized numerical computations, memory efficiency, and broadcasting capabilities. As pandas is built on top of numpy, we can easily extract the vectors and work easily with them.\n",
    "\n",
    "> <details>\n",
    "> <summary>Sorry, you said broadcasting?</summary>\n",
    "> \n",
    "> Yes, we said broadcasting.\n",
    "> \n",
    "> Broadcasting is like a smart way for NumPy to handle math with arrays of different sizes. It helps make operations work even if the arrays aren't the exact same shape. This makes it easier to do calculations without having to write lots of extra code.\n",
    ">\n",
    "> Curious about it? Take a look here:\n",
    ">\n",
    "> [https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "> </details>\n",
    "\n",
    "So, that's what we are going to do:\n",
    "\n",
    "- We will store the numerical values from our dataframes in numpy ndarrays.\n",
    "- Then we will calculate the distance (or similarity) of our query vector to (or with) all the other vectors.\n",
    "- We will create a copy of our original data dataframe and add a column for each metric.\n",
    "- Finally, for helping us to better understand the model, we will add another column that shows the tokens present both in the query in every specific film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the numpy ndarrays from our dataframes\n",
    "query_vector = query_df.to_numpy()\n",
    "bow_matrix = bow_df.to_numpy()\n",
    "\n",
    "# calculating the distances\n",
    "euclid_distances = euclid_dist_AB = np.linalg.norm(bow_matrix - query_vector, axis=1)\n",
    "dotprod_similarities = np.dot(bow_matrix, query_vector.T)\n",
    "cos_similarities = cosine_similarity(query_vector, bow_matrix).flatten()\n",
    "\n",
    "# creating the new dataframe and adding the extra columns\n",
    "results = df.loc[:, ['title', 'descriptor']].copy()\n",
    "results.loc[:, 'euclid_dist'] = euclid_distances\n",
    "results.loc[:, 'dot_prod_sim'] = dotprod_similarities\n",
    "results.loc[:, 'cos_sim'] = cos_similarities\n",
    "results.loc[:, 'common_tokens'] = df.loc[:, 'tokens'].map(lambda x: list(set(x).intersection(query_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the best results by sorting the dataframe by metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric! options:\n",
    "# 'euclid_dist', 'dot_prod_sim', 'cos_sim'\n",
    "metric = 'euclid_dist'\n",
    "\n",
    "# choose number of results\n",
    "n = 10\n",
    "\n",
    "# show the results\n",
    "results.sort_values(by=metric, ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you curious about the descriptor for the film we were looking for? Well, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.title == \"The Shawshank Redemption\", 'descriptor'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Feel free to look for other films using your own queries, or just let the system recommend you films based on what you would like to see. And try to answer this questions:\n",
    "- How good are the recommendations?\n",
    "- How close are the suggested films to what you were loooking for?\n",
    "- Which metrics perform better for this model?\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>(Optional) Exercise</strong>\n",
    "\n",
    "Try to change the code from the last two chapters to turn the binary Bag of Words into a Term Frequency Bag of Words. It involves changing just a couple of things and it's done.\n",
    "\n",
    "Is the tf-BoW better than the binary BoW? What do you think?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>5. (Optional) Bag of Words with the <kbd>gensim</kbd> package\n",
    "</div>\n",
    "\n",
    "We could do the same in a more efficient way with libraries specifically developed to deal with texts. One example for this is <gensim>. Gensim is a Python library designed for topic modeling and document similarity analysis. It provides tools for building, training, and using topic models such as Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). It is widely used for tasks like extracting topics from large text corpora, document similarity comparison, and creating vector representations of words and documents using techniques like Word Embeddings (Word2Vec) as we will see later.\n",
    "\n",
    "> <details>\n",
    "> <summary>More options</summary>\n",
    "> Of course, there are way many more options. If you want to explore a bit further the Bag of Words universe, you could take a look to <kbd>tmtoolkit</kbd>, a library developed for text mining and topic modeling. You can also combine this library with <kbd>gensim</kbd> very easily.\n",
    ">\n",
    "> You can find more Info about tmtoolkit here:\n",
    "> [https://tmtoolkit.readthedocs.io/en/latest/bow.html](https://tmtoolkit.readthedocs.io/en/latest/bow.html)\n",
    "> </details>\n",
    "\n",
    "We are going to repeat the same steps we did before for creating a Bag of Words model but using <kbd>gensim</kbd>. This time is going to be a Term Frequency Bag of Words. The metric per default in <kbd>gensim</kbd> is the cosine similarity. \n",
    "\n",
    "Let's start creating a dictionary with the tokens and then the corpus. The dictionary will contain all the unique tokens so it will work as our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create a Dictionary object from the tokens\n",
    "# store a copy in the artifacts folder\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "dictionary.save('../artifacts/descriptors.dict')\n",
    "\n",
    "# Create the corpus with a vector for each document\n",
    "corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our next step we need to create a (temporary) index for fast access. For that we will use the <kbd>Similarity</kbd> class.\n",
    "\n",
    "> <details>\n",
    "> <summary>Hmm, but what's indexing?</summary>\n",
    "> \n",
    "> Indexing is a common technique that allows us to create an optimized data structure for quick access to information, enhancing data retrieval speed. It facilitates efficient search, sorting, and supports constraints, significantly improving overall system performance and responsiveness. In databases, indexes are crucial for optimizing query execution and join operations.\n",
    ">\n",
    "> But don't worry! At the end of the workshop we will take a look at indexing with a bit more of detail.\n",
    "> </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# get the path to the temporary file 'index'\n",
    "index_tmpfile = get_tmpfile(\"index\")\n",
    "\n",
    "# create the index instantiating the class Similarity\n",
    "index = Similarity(index_tmpfile, corpus, num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our next step, we write down our query and tokenize it as we have done before. Then it needs to be vectorized.\n",
    "\n",
    "We can then use our index to find the cosine similarity between the query and all the rest of descriptors in the corpus.\n",
    "\n",
    "Finally we add the cosine similarities to a column in a copy of the original dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"An innocent man in prison that never loses the hope starts helping the warden as accountant\"\n",
    "\n",
    "# normalizing the query\n",
    "query_tokens = normalize(query, tkn, punct_signs=True)[1]\n",
    "query_bow = dictionary.doc2bow(query_tokens)\n",
    "query_bow\n",
    "\n",
    "# get cosine similarity between\n",
    "# the query and all index documents\n",
    "cos_similarities = index[query_bow]\n",
    "\n",
    "# creating the dataframe\n",
    "results = df.loc[:, ['title', 'descriptor']].copy()\n",
    "results.loc[:, 'cos_sim'] = cos_similarities\n",
    "results.loc[:, 'common_tokens'] = df.loc[:, 'tokens'].map(lambda x: list(set(x).intersection(query_tokens)))\n",
    "\n",
    "# show the results\n",
    "results.sort_values(by='cos_sim', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "As in the previous chapter, feel free to look for other films using your own queries, or just let the system recommend you films based on what you would like to see. And try to answer this questions:\n",
    "- How good are the recommendations from this BoW created with <kbd>gensim</kbd>?\n",
    "- Are they better than the ones we coded ourselves?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>6. Advantages and disadvantages of BoW Models\n",
    "</div>\n",
    "\n",
    "Let's talk about the pros and cons of BoW models, and see where can they be used.\n",
    "\n",
    "#### Advantages:\n",
    "\n",
    "> - **Simplicity**: Bag of Words (BoW) models excel in simplicity, offering an intuitive representation of text based on word frequency. This simplicity makes BoW ideal for straightforward implementation and quick deployment in various natural language processing (NLP) tasks.\n",
    "> \n",
    "> - **Versatility**: BoW models exhibit versatility, making them adaptable to a spectrum of NLP applications. Their simplicity allows for easy customization, making them suitable for tasks such as text classification, sentiment analysis, and information retrieval.\n",
    "> \n",
    "> - **Efficiency**: BoW representations, especially in sparse formats, contribute to computational efficiency. This efficiency becomes crucial when dealing with large text corpora, enabling the processing of extensive datasets without an overwhelming demand for computational resources.\n",
    "> \n",
    "> - **Interpretability**: BoW models offer interpretable representations. Each element in the BoW vector corresponds to the frequency or presence of a specific word, providing a clear understanding of the contributing factors to the model's output.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "> - **Loss of Word Order**: A notable limitation of BoW models is the neglect of word order and grammar. This oversight results in the loss of crucial sequential information present in the text, making BoW less suitable for tasks reliant on context.\n",
    "> \n",
    "> - **Sparsity**: BoW vectors can become highly sparse in high-dimensional spaces, posing challenges in terms of memory efficiency. The sparsity may necessitate additional computational resources for storage and processing.\n",
    "> \n",
    "> - **Lack of Semantics**: BoW models lack the ability to capture semantic relationships between words. This limitation makes them less effective for tasks requiring an understanding of nuanced meanings and contextual significance.\n",
    "> \n",
    "> - **Vocabulary Size**: Managing and processing extensive vocabularies can be a consideration, particularly when dealing with large and diverse datasets. The size of the vocabulary impacts both memory and computational efficiency.\n",
    "> \n",
    "\n",
    "#### Applications:\n",
    "\n",
    "> - **Text Classification**: In text classification tasks, such as sentiment analysis or spam detection, Bag of Words (BoW) models serve as a fundamental representation. Multinomial Naive Bayes classifiers are often the method of choice in this context. These classifiers operate on the BoW vectors, leveraging word frequencies to calculate probabilities and make predictions about the category or sentiment of a given document. The simplicity of BoW complements the assumptions of Naive Bayes, making this combination effective for quick and interpretable text classification.\n",
    "> \n",
    "> - **Information Retrieval**: For information retrieval applications, where the goal is to build efficient search engines, BoW models find utility in Vector Space Models (VSM). BoW vectors, representing documents and queries, are used to calculate the cosine similarity, serving as a measure of relevance. This method allows search engines to quickly retrieve and rank documents based on their similarity to a given search query, making BoW a foundational component in information retrieval systems.\n",
    ">\n",
    "> - **Topic Modeling**: In tasks related to topic modeling, such as uncovering latent themes within a collection of documents, BoW models play a crucial role when combined with methods like Latent Dirichlet Allocation (LDA). LDA utilizes BoW representations to identify topics by analyzing the distribution of words across documents. BoW's ability to capture word frequencies is essential for LDA to uncover the underlying thematic structure within the corpus.\n",
    ">\n",
    "> - **Baseline Models**: As baseline models in natural language processing, BoW, when paired with Naive Bayes classifiers, offers a simple and effective solution for various applications. Whether it's a quick assessment of sentiment or classifying documents into predefined categories, the straightforwardness of BoW representations aligns well with the assumptions of Naive Bayes. This combination serves as a practical starting point in NLP tasks, providing a balance between simplicity and reasonable performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
