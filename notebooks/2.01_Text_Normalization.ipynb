{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "<img src=\"../media/img/kisz_logo.png\" width=\"192\" height=\"69\"> \n",
    "<h1> Working with embeddings:\n",
    "<h2>An introductory workshop with applications on Semantic Search\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f6a800; color: #ffffff; padding: 10px;\">\n",
    "<h2>Part 2.1 - Text Normalization\n",
    "</div>\n",
    "\n",
    "In this part we are going to see how to turn texts into a collection of smaller pieces, tokens, that we will use for building numerical representations of our texts. In our way to find good ways of tokenizing texts we will be facing some common problems and we will discuss possible solutions. At the end, we will build a pipeline for tokenizing and serialize the pipeline and the tokenized texts.\n",
    "\n",
    "We start importing some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import config variables for the notebooks\n",
    "from nb_config import RAW_DATA_PATH, INTERIM_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>1. Overview\n",
    "</div>\n",
    "\n",
    "We are going to work with real data and with a concrete problem in mind: retrieving information from a collection of objects, each described with a text, based on a query written by the user. Let's formulate the problem in a more precise way.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Challenge</strong>\n",
    "\n",
    "\n",
    "As a developer for the cutting-edge movie platform HBFlix, your task is to implement a semantic search feature that enables users to input a description or query, and in return, receive a list of films that match as close as possible the query based on semantic similarities. The movie database at your disposal consists of around three thousand films, each accompanied by release year and a concise publicity descriptor of the movie.\n",
    "</div>\n",
    "\n",
    "The dataset is a reduced version of [Kaggle's The Movies Dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset). We have kept only film names, release years and film descriptors. Also, because some of the models that we are going to use don't perform well on big datasets, we have filtered out.\n",
    "\n",
    "As we are going to make a lot of experiments with this data, we will save the dataframe in parquet format for easy access. We will also rename the column 'overview' with the name 'descriptor'.\n",
    "\n",
    "><details>\n",
    "><summary>Do you need more data?</summary>\n",
    ">You can load a bigger unfiltered version of this dataset, with the same structure but including descriptors for around 45.000 films with this code:\n",
    ">\n",
    "><code>full_df = load_dataset('mt0rm0/movie_descriptors', split='train').to_pandas()</code>\n",
    "></details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = load_dataset('mt0rm0/movie_descriptors_small', split='train').to_pandas()\n",
    "df.rename(columns={'overview': 'descriptor'}, inplace=True)\n",
    "\n",
    "# save the dataframe in parquet format\n",
    "df.to_parquet(RAW_DATA_PATH + 'movie_descriptors.parquet')\n",
    "\n",
    "# show the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>2. Tokenization\n",
    "</div>\n",
    "\n",
    "Tokenization is the process of breaking down a text or a sequence of characters into smaller units, often words or subwords, referred to as tokens. In Natural Language Processing (NLP), tokenization is a crucial step in preparing textual data for analysis. The resulting tokens serve as the basic building blocks for various NLP tasks, allowing algorithms to process and understand the structure of the text.\n",
    "\n",
    "Let's start taking a look to the texts we are going to work with, the descriptors for the movies in our list. The first one looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.descriptor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get our hands dirty!\n",
    "\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Let's keep it easy and just split the text using whitespaces as reference. Each splitted string will be a token. Add then to our dataframe a column called *tokens* that contains the list of tokens for each movie.\n",
    "</div>\n",
    "\n",
    "Write your code in the cell below.\n",
    "\n",
    "><details>\n",
    "><summary>Do you need some help?</summary>\n",
    "><br>\n",
    ">You can split strings in python with the <kbd>split()</kbd> method applied to a string.\n",
    "></details>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "><details>\n",
    "><summary>Maybe a bit more of help?</summary>\n",
    "><br>\n",
    ">You can apply a method elementwise to a series or dataframe column with the <kbd>map()</kbd> method and a lambda function.\n",
    "></details>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "> <details>\n",
    "> <summary>Got completely stuck? Here there are some possible solutions</summary>\n",
    "> \n",
    "> This line of code would work:<br>\n",
    "> <code>df['tokens'] = df['descriptor'].map(lambda x: x.split(' '))</code>\n",
    "> </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = ... # Your solution here\n",
    "\n",
    "# Show the tokens\n",
    "df.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the output for the first descriptor, we can already see some problems with this method...\n",
    "\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Take a look to the output for other descriptors. Then, take a couple of minutes to discuss with your neighbour where have you found problems and how would we fix them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index in the next line (any number from 0 to 2864 would do)\n",
    "# to check the tokens for other films\n",
    "print(df.tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems found**: \n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "**How to solve these problems**:\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>3. Ways to improve our tokens\n",
    "</div>\n",
    "\n",
    "We will focus now on how to improve our tokens so we can extract as much information as possible from them.\n",
    "\n",
    "For making it a bit easier for you, we have implemented a better version of the simple tokenizer that you have written in the last section. This tokenizer accounts for some of the problems, like:\n",
    "- newline escape characters\n",
    "- commas and dots at the end of a word\n",
    "- empty strings/tokens\n",
    "\n",
    "You can import this tokenizer from <kbd>src.normalizing</kbd>. If you want, Feel free to take a look to the code and try to understand how <kbd>SimpleTokenizer</kbd> works.\n",
    "\n",
    "The following code shows how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.normalizing import SimpleTokenizer\n",
    "\n",
    "# instantiate the SimpleTokenizer\n",
    "st = SimpleTokenizer()\n",
    "\n",
    "# apply the tokenizer to our text\n",
    "df.loc[:, 'tokens'] = df.descriptor.map(lambda x: st.tokenize(x))\n",
    "\n",
    "# show the tokens\n",
    "df.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that they are still not perfect, but it looks much better.\n",
    "\n",
    "There are, of course, a lot of ways for tokenizing our texts, but we don't even need to do it ourselves. Examples of common tokenizer implementations are:\n",
    "- **Python**: str.split, re.split\n",
    "- **NLTK**: PennTreeBankTokenizer, TweetTokenizer\n",
    "- **spaCy**: Tokenizer class, fully customizable\n",
    "- **Stanford CoreNLP**: linguistically accurate, requires Java interpreter\n",
    "- **Huggingface**: BertTokenizer\n",
    "\n",
    "We have prepared for you in the module <kbd>src.normalizing</kbd> a standard Spacy tokenizer and the NLTK PennTreeBank tokenizer.\n",
    "You can find them with the names <kbd>SpaCyTokenizer</kbd> and <kbd>NLTKTokenizer</kbd>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.normalizing import SpaCyTokenizer, NLTKTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how to use them in the next lines. Tokenizing the whole data set would take quite a few minutes so we are going to check only the tokens for the first movie.\n",
    "\n",
    "> <details>\n",
    "> <summary>About the SpacyTokenizer</summary>\n",
    "> \n",
    ">  We have preloaded only the small SpaCy tokenizer, but you could load tokenizers of different sizes using as parameter <kbd>size='md'</kbd> for medium size or <kbd>size='lg'</kbd> for the largest model, when instantiating the SpacyTokenizer as shown:\n",
    ">\n",
    "> <pre><code># use 'sm' for small (default)\n",
    "> # 'md' for medium or 'lg' for large  \n",
    "> tokenizer = SpaCyTokenizer(size='lg')\n",
    "> </code></pre>\n",
    "> \n",
    "> </details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer you want to use: SpacyTokenizer or NLTKTokenizer \n",
    "tokenizer = SpaCyTokenizer()\n",
    "\n",
    "# Apply the tokenizer to the first abstract as example\n",
    "tokens = tokenizer.tokenize(df.descriptor[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>4. Lemmatization and stemming\n",
    "</div>\n",
    "\n",
    "**Lemmatization** is a technique that involves reducing words to their base or canonical form, known as the \"lemma.\" The lemma represents the dictionary form or the base form of a word, which is often a valid word that can be found in the language's dictionary.\n",
    "\n",
    "Lemmatization is different from **stemming**, another text normalization technique. While stemming involves cutting off prefixes or suffixes of words to obtain a root form (which may not always be a valid word), lemmatization aims to transform words to their base form, preserving their meaning and ensuring that the resulting lemma is a valid word in the language.\n",
    "\n",
    "For example:\n",
    "- The lemma of \"running\" is \"run\" and the stem is \"run\"\n",
    "- The lemma of \"better\" is \"good\" and the stem is \"bett\" (stemming doesn't always produce valid words)\n",
    "- The lemma of \"mice\" is \"mouse\" and the stem is \"mice\"\n",
    "\n",
    "We have prepared a lemmatizer and some stemmers as alternatives to our simple tokenizer:\n",
    "\n",
    "The SpaCy lemmatizer is implemented as part of the SpaCyTokenizer in the module <kbd>src.normalizing</kbd>. **Instead of** using the method <kbd>tokenize</kbd>, you can use the method <kbd>lemmatize</kbd>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SpacyTokenizer \n",
    "tokenizer = SpaCyTokenizer()\n",
    "\n",
    "# Apply the lemmatizer to the first abstract as example\n",
    "tokens = tokenizer.lemmatize(df.descriptor[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stemmers work on a token list and can be found as methods of the class <kbd>WordTools</kbd> in the <kbd>src.utils</kbd> module.\n",
    "\n",
    "Two different stemmers have been implemented:\n",
    "- The Porter stemmer (<kbd>porter_stemmer()</kbd>)\n",
    "- The Snowball stemmer (<kbd>snowball_stemmer()</kbd>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import WordTools\n",
    "\n",
    "# Instantiate the SpacyTokenizer \n",
    "tokenizer = SpaCyTokenizer()\n",
    "\n",
    "# Apply the lemmatizer to the first abstract as example\n",
    "tokens = tokenizer.tokenize(df.descriptor[0])\n",
    "\n",
    "# Apply the stemmer\n",
    "tokens = WordTools.porter_stemmer(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>5. Stop words\n",
    "</div>\n",
    "\n",
    "Stop words are common words in a language that are often filtered out during text preprocessing because they are considered to be of little value in terms of conveying meaning. Examples of stop words in English include \"the,\" \"and,\" \"is,\" \"of,\" etc. Stop words can vary depending on the language.\n",
    "\n",
    "In natural language processing, it's common to remove stop words from text data to reduce noise and improve the efficiency of downstream tasks. The NLTK library in Python provides a list of common stop words for various languages.\n",
    "\n",
    "We provide a tool for removing stop words using the NLTK stop word filter. You can find it as method of the class <kbd>WordTools</kbd> in the <kbd>src.utils</kbd> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SpacyTokenizer \n",
    "tokenizer = SpaCyTokenizer()\n",
    "\n",
    "# Apply the lemmatizer to the first abstract as example\n",
    "tokens = tokenizer.tokenize(df.descriptor[0])\n",
    "\n",
    "# Apply the stemmer\n",
    "tokens = WordTools.stopword_filter(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>6. Case folding\n",
    "</div>\n",
    "\n",
    "Case folding is a text normalization technique that involves converting all the characters in a piece of text to a common, usually lowercase, form. The purpose of case folding is to ensure consistency and facilitate comparisons, as it makes the text case-insensitive.\n",
    "\n",
    "You can find a tool for case folding as method of the class <kbd>WordTools</kbd> in the <kbd>src.utils</kbd> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SpacyTokenizer \n",
    "tokenizer = SpaCyTokenizer()\n",
    "\n",
    "# Apply the lemmatizer to the first abstract as example\n",
    "tokens = tokenizer.tokenize(df.descriptor[0])\n",
    "\n",
    "# Apply the stemmer\n",
    "tokens = WordTools.case_folding(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>7. Building our own normalization pipeline\n",
    "</div>\n",
    "\n",
    "We can choose now some of these tools to create our tokens.\n",
    "\n",
    "As the SpaCy and NLTK tokenizers take considerably more time than the simple tokenizer, we are going to stick to this last one for now. We will not stem or lemmatize but we will use case folding and remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SimpleTokenizer\n",
    "st = SimpleTokenizer()\n",
    "\n",
    "# Apply the tokenizer to our text\n",
    "print(\"Tokenizing...\")\n",
    "%timeit -r1 df.loc[:, 'tokens'] = df.descriptor.map(lambda x: st.tokenize(x))\n",
    "\n",
    "# Remove the stop words\n",
    "print(\"Removing stop words...\")\n",
    "%timeit -r1 df.loc[:, 'tokens'] = df.descriptor.map(lambda x: WordTools.stopword_filter(x))\n",
    "\n",
    "# Case folding\n",
    "print(\"Case folding...\")\n",
    "%timeit -r1 df.loc[:, 'tokens'] = df.descriptor.map(lambda x: WordTools.case_folding(x))\n",
    "\n",
    "# Removing punctuation signs\n",
    "print(\"Removing punctuation signs...\")\n",
    "%timeit -r1 df.loc[:, 'tokens'] = df.descriptor.map(lambda x: WordTools.punct_remover(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a way of tokenizing our texts, and we have applied that system to all our texts. Is that enough?\n",
    "\n",
    "Well, the answer is no.\n",
    "\n",
    "We will also want to be able to tokenize our queries using exactly the same method we used for tokenize the descriptors, right?\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Create a function called <kbd>pipeline</kbd> that gets as input a text and returns as ouput a list of tokens following the same steps we used before.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "> <details>\n",
    "> <summary>Not working at all? Take a look here</summary>\n",
    "> \n",
    "> This could be a basic implementation of the function:\n",
    "> <pre><code>\n",
    "> def pipeline(text:str):\n",
    ">     # instantiate the SimpleTokenizer\n",
    ">     st = SimpleTokenizer()<br>\n",
    ">     # apply the tokenizer to our text\n",
    ">     tokens = st.tokenize(text)<br>\n",
    ">     # remove the stop words\n",
    ">     tokens = WordTools.stopword_filter(tokens)<br>\n",
    ">     # case folding\n",
    ">     tokens = WordTools.case_folding(tokens)<br>\n",
    ">     # Removing punctuation signs\n",
    ">     tokens = WordTools.punct_remover(tokens)<br>\n",
    ">     return tokens\n",
    "> </code></pre>\n",
    "> </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here below\n",
    "def pipeline(text: str) -> list[str]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>8. Automatizing the process\n",
    "</div>\n",
    "\n",
    "Do you want to experiment with different pipelines by implementing different combinations of the functions before? \n",
    "\n",
    "No worries. We have prepared for you a function called <kbd>normalize()</kbd> in the module <kbd>src.normalizing</kbd>. With that function you can choose different settings for your pipeline by adjusting the input parameters. It will output for each text a dictionary with the parameters and a list with the tokens.\n",
    "\n",
    "You can see with the next code how does it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.normalizing import normalize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "df = pd.read_parquet(RAW_DATA_PATH+'movie_descriptors.parquet')\n",
    "\n",
    "# instantiate the SimpleTokenizer\n",
    "tkn = NLTKTokenizer()\n",
    "\n",
    "# normalize the texts\n",
    "%timeit -r1 df.loc[:, 'tokens'] = df.descriptor.map(lambda x: normalize(x, tkn=tkn, punct_signs=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to save your tokenized data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(INTERIM_DATA_PATH+'my_tokenized_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>9. Optional: Keeping track of the tokenized data\n",
    "</div>\n",
    "\n",
    "If we want to make experiments with different settings, it would be a great idea to keep track of the settings we use for each configuration, so we can always compare different settings for looking for the best performing combination.\n",
    "\n",
    "> <details>\n",
    "> <summary>About tokenization tracking</summary>\n",
    "> \n",
    "> Tokenization tracking is part of a wider concept called artifact tracking. In the context of MLOps, artifact tracking involves managing and versioning various artifacts, such as trained models, datasets, and preprocessing scripts. It ensures reproducibility and traceability of experiments.\n",
    ">\n",
    "> Tokenization information, along with other preprocessing steps, can be considered artifacts. Tracking these artifacts allows you to understand how data was transformed and processed before being used in model training.\n",
    "> </details>\n",
    "\n",
    "One way of doing it would be by automatically saving metadata about the tokenization process every time we serialize one tokenized dataset to parquet. That way we can reconstruct the same tokenizer configuration we used originally and reuse it for queries or for training even more data.\n",
    "\n",
    "We have implemented as an example the function <kbd>df_pipeline()</kbd> in <kbd>src.normalizing</kbd> that gets as input a dataframe, and the same parameters as the <kbd>normalize()</kbd> function we ave seen before and out puts the dictionary with the parameters and the dataframe with the tokens in the column tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.normalizing import df_pipeline\n",
    "\n",
    "params, df = df_pipeline(df, tkn=tkn, punct_signs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass the dataframe, a file name (wihtout extension) and the parameters dictionary to <kbd>data_logger()</kbd> in <kbd>src.data</kbd> and this function will store the data as a parquet file in <code>data/interim/</code> with the given name, and will store (or update))the metadata in the file <code>data/data.json</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_logger\n",
    "\n",
    "data_logger(df, \"my_tokenized_data.parquet\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can later extract the data and the metadata with the following code:\n",
    "\n",
    "<code>from src.data import data_loader\n",
    "\n",
    "df, params = data_loader(\"my_tokenized_data.parquet\")</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
