{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "<img src=\"../media/img/kisz_logo.png\" width=\"192\" height=\"69\"> \n",
    "<h1> Working with embeddings:\n",
    "<h2>An introductory workshop with applications on Semantic Search\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f6a800; color: #ffffff; padding: 10px;\">\n",
    "<h2>Part 3.1 - Word2Vec\n",
    "</div>\n",
    "\n",
    "In this section we will take a look to the Word2Vec family and its two different approaches. We will use <kbd>gensim</kbd> for training from scratch a model and we'll compare it with a pretrained model, using different tools and utilities included in <kbd>gensim</kbd>. Back to our use case, we will see how our movie search engine performs with Word2Vec.\n",
    "\n",
    "We start as usual importing some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nb_config import INTERIM_DATA_PATH\n",
    "\n",
    "from src.data import data_loader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>1. Overview\n",
    "</div>\n",
    "\n",
    "Word2Vec is a popular word embedding model that captures semantic relationships between words in a corpus. Unlike traditional methods such as BoW or TF-IDF, Word2Vec embeds words into continuous vector spaces, preserving their semantic meanings and relationships.\n",
    "\n",
    "To create a Word2Vec model, two main architectures are commonly employed, Continuous Bag of Words (CBOW) and Skip-gram. Both approaches involve training a neural network on a large dataset, typically comprising sentences or documents, to learn word embeddings. These embeddings are vectors that position words in a multi-dimensional space based on their contextual similarities.\n",
    "- In the CBOW architecture, the model predicts the target word given its context, or the surrounding words.\n",
    "- The Skip-gram architecture predicts the context words based on a given target word.\n",
    "\n",
    "Training the Word2Vec model involves adjusting the vector representations of words to minimize the difference between predicted and actual words. Once the Word2Vec model is trained, the resulting word vectors can be used to measure semantic relationships between words. Words with similar meanings or contexts will have vectors that are close together in the vector space. This allows for semantic operations, such as vector subtractions, where the vector for \"king\" minus \"man\" plus \"woman\" may yield a vector close to \"queen.\"\n",
    "\n",
    "We are going to need at this point our tokens. Let's load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, params = data_loader(\"my_tokenized_data.parquet\")\n",
    "\n",
    "# extract the arguments for the normalize function\n",
    "tokenizer = params['tokenizer']\n",
    "arguments = params['args']\n",
    "\n",
    "# print the tokenizer used getting the tokens\n",
    "print(f\"Tokenizer used: {tokenizer}\")\n",
    "print(f\"Arguments passed to the normalize function:\\n{arguments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>2. Training your own word2vec embeddings with <kbd>gensim</kbd>\n",
    "</div>\n",
    "\n",
    "Let's train a Word2Vec model with our data. We will just use our tokenized descriptors as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# pack the tokens as a list of lists\n",
    "tokens = df.tokens.apply(lambda x: x.tolist()).tolist()\n",
    "\n",
    "# instantiate the model with our data as training corpus\n",
    "model = Word2Vec(sentences=tokens, sg=1, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# proveide fast access to the vectors\n",
    "w2v_1 = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voilà! We have created a Word2Vec model trained from us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>3. Using pretrained word2vec embeddings with <kbd>gensim</kbd>\n",
    "</div>\n",
    "\n",
    "We can also used pre-trained embeddings. They have been trained on large and diverse datasets, so they can usually capture general semantic relationships and linguistic patterns better than our generated embeddings. Using pretrained embeddings also saves resources and time, as you can leverage embeddings that have already been trained on powerful hardware and large-scale datasets.\n",
    "\n",
    "We will start taking a look to the different embeddings that gensim can offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Show all available models in gensim-data\n",
    "print(*list(api.info()['models'].keys()), sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the <kbd>word2vec-google-news-300</kbd> embeddings. This model was trained on a part of the Google News dataset (about 100 billion words) and contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in the paper ['Distributed Representations of Words and Phrases and their Compositionality'](https://arxiv.org/abs/1310.4546).\n",
    "\n",
    "\n",
    "> <details>\n",
    "> <summary>Too small for you?</summary>\n",
    "> You can also try the [word2vec model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) released by Google. It’s 1.5GB and includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset.\n",
    ">\n",
    "> You can even load this model with gensim. Download it, unzip it the <kbd>data/external</kbd> folder and use following code to get started:\n",
    "> \n",
    "> <pre><code>from gensim.models.keyedvectors import KeyedVectors\n",
    "> # load the model\n",
    ">w2v_3 = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "> </code></pre>\n",
    "> \n",
    "> </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>4. Working with the embeddings\n",
    "</div>\n",
    "\n",
    "The embeddings in the <kbd>w2v_1</kbd> and <kbd>w2v_2</kbd> objects can be accessed exactly as if they were a dictionary, using as keys the different tokens.\n",
    "\n",
    "```\n",
    "        w2v_1['<token>']\n",
    "```\n",
    "\n",
    "Working with Gensim has some benefits. This libary provides a lot of useful methods to work with these embeddings. Let's see some of them!\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "You will find in the following cells different functionalities of <kbd>gensim</kbd>. Check them for your two models:\n",
    "- **<kbd>w2v_1</kbd>**, trained on our data\n",
    "- **<kbd>w2v_2</kbd>**,  loaded from pre-trained embeddings\n",
    "\n",
    "You can also change the examples or tokens used for each functionality to get a better understanding of their strenghts and limitations. \n",
    "\n",
    "How are the results for both models? What tasks is each model good at?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Cosine Similarity**: We can directly measure the (cosine) similarity between two tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens to compare\n",
    "compared_tokens = ['woman', 'man']\n",
    "\n",
    "# calculating the cosine similarity\n",
    "similarity = w2v_1.similarity(*compared_tokens)\n",
    "print(f\"Similarity of {compared_tokens} is {similarity:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Finding similar tokens**: We can look for the most similar tokens, given a single token as reference. The result provides a list with the 10 closest tokens. We will show only the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = w2v_1.similar_by_word(\"cat\")\n",
    "print(\"{}: {:.6f}\".format(*result[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Word Analogies**:  We can look for the most similar tokens, given not only one but several tokens as reference, in a way that can be used for analogy questions. As example, we may want to know what would be for *woman*, what *king* is for *man*. We can use the <kbd>most_similar</kbd> method. As before, the result provides a list with the 10 closest tokens and we will show only the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens = ['woman', 'king']\n",
    "negative_tokens = ['man']\n",
    "\n",
    "result = w2v_1.most_similar(positive=positive_tokens, negative=negative_tokens)\n",
    "print(\"{}: {:.6f}\".format(*result[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **The odd one out**: We can compare tokens to see which one is less like the others. For example, if we provide the tokens *cat*, *dog*, *horse*, *fridge* and *cow*, we would expect the token *fridge* to be more different than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_list = ['cat', 'dog', 'horse', 'fridge', 'cow']\n",
    "result = w2v_1.doesnt_match(match_list)\n",
    "print(f\"In the list {match_list} the token '{result}' doesn't match\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Sentence comparison**: We can compare sentences. For that we tokenize the words of each sentence and pass the tokens lists to the model as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"Peter's parents went with him for a walk\".lower().split()\n",
    "\n",
    "sentence_2 = \"The couple went with their son Peter for a walk\".lower().split()\n",
    "\n",
    "sim = w2v_1.n_similarity(sentence_1, sentence_2)\n",
    "print(\"{:.6f}\".format(sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise (optional)</strong>\n",
    "\n",
    "Did you had any problem with tokens that were not in the vocabulary? How did <kbd>gensim</kbd> handle it? How would you handle it?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>5. Queries with Word2Vec\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will come back now to our original task: building a Semantic Search Engine. We are going to check now how our new models perform with that task.\n",
    "\n",
    "> **NOTE**: As the model with pre-trained embeddings performs much better than the one we trained with our data, we will use <kbd>w2v_2</kbd> as standard. Feel free to try the queries also with the other model if you want.\n",
    "\n",
    "But... Word2Vec produces word embeddings and not sentence or text embeddings. How can we get embeddings for our descriptors? We have many different options, but at this point the most common solutions would be:\n",
    "\n",
    ">- **Average of Word2Vec vectors** : You can just take the average of all the word vectors in a sentence. This average vector will represent your sentence vector. That's the usual approach and the one will be doing here.\n",
    ">- **Average of Word2Vec vectors with TF-IDF**: Using TF-IDF with this Word2Vec embeddings we could leverage the importance and thus the weight of the different tokens. We will leave that as a homework by now. \n",
    ">- **Doc2Vec** : you can train your dataset using Doc2Vec and then use the sentence vectors. We will actually be doing this in the **Notebook 3.4.- Doc2Vec**, so we don't have to worry about it by now.\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Create the embeddings for the descriptors in our dataset, by averaging the vectors for the tokens in each descriptor. Present the result as a matrix with so many rows as movies)and so many columns as dimensions in our embeddings, and store it in a variable called <kbd>corpus_matrix</kbd>.\n",
    "\n",
    "Make sure that out of dictionary tokens don't break the code.\n",
    "</div>  \n",
    "<br>\n",
    "\n",
    "> <details>\n",
    "> <summary>Need some help?</summary>\n",
    "> \n",
    "> Take a look to the [API](https://radimrehurek.com/gensim/apiref.html) of Word2Vec models by <kbd>gensim</kbd>. It could also help knowing that in this library Word2Vec models are a type of *keyedvectors*...\n",
    "> </details>\n",
    "<br>\n",
    "\n",
    "> <details>\n",
    "> <summary>Still clueless?</summary>\n",
    ">\n",
    "> Here is some code that could do the trick:\n",
    ">\n",
    "> <pre><code># creates a Pandas series with the averaged vectors\n",
    "> corpus_matrix = df.loc[:, 'tokens'].map(lambda x: w2v_2.get_mean_vector(x, ignore_missing=True))\n",
    "> # creates the matrix from the vectors\n",
    "> corpus_matrix = np.vstack(corpus_matrix)\n",
    "> </code></pre>\n",
    "> \n",
    "> </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a Pandas series with the averaged vectors\n",
    "corpus_matrix = ...\n",
    "# creates the matrix from the vectors\n",
    "corpus_matrix = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of our corpus matrix: {corpus_matrix.shape}\\n\")\n",
    "corpus_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Create the embedding of the query, and find its distance to the other vectors following the next steps:\n",
    "1. Tokenize the query with the right tokenizer and parameters\n",
    "2. Calculate the averaged embedding from the token vectors\n",
    "3. Reshape the query vector to turn it into a 2-dimensional array, by adding an extra dimension behind\n",
    "4. Calculate the euclidean distance, dot product similarity and cosine similarity as we have done before\n",
    "</div>  \n",
    "<br>\n",
    "\n",
    "> <details>\n",
    "> <summary>If you get lost...</summary>\n",
    "> \n",
    "> We don't think you should have problems with steps 1. and 4. as you have done it already several times in the past notebooks, but for the second and third step, that culd help...\n",
    ">\n",
    "> <pre><code># creating the averaged vector\n",
    "> query_vector = w2v_2.get_mean_vector(query_tokens, ignore_missing=True)\n",
    "> # reshaping the array \n",
    "> query_vector = query_vector.reshape((1, -1))\n",
    "> </code></pre>\n",
    "> </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"An innocent man in prison that never loses the hope starts helping the warden as accountant\"\n",
    "\n",
    "# code for tokenizing the query\n",
    "\n",
    "query_tokens = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for calculating the embeddings\n",
    "query_vector = ...\n",
    "\n",
    "# code for reshaping the array\n",
    "query_vector = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the distances\n",
    "euclid_distances = ...\n",
    "dotprod_similarities = ...\n",
    "cos_similarities = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create a dataframe to store all those values, as we did in the last notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new dataframe and adding the extra columns\n",
    "results = df.loc[:, ['title', 'descriptor']].copy()\n",
    "results.loc[:, 'euclid_dist'] = euclid_distances\n",
    "results.loc[:, 'dot_prod_sim'] = dotprod_similarities\n",
    "results.loc[:, 'cos_sim'] = cos_similarities\n",
    "results.loc[:, 'common_tokens'] = df.loc[:, 'tokens'].map(lambda x: list(set(x).intersection(query_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise</strong>\n",
    "\n",
    "Which metrics show better results? You can try several different queries with varying degrees of complexity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric! options:\n",
    "# 'euclid_dist', 'dot_prod_sim', 'cos_sim'\n",
    "metric = 'cos_sim'\n",
    "\n",
    "# choose number of results\n",
    "n = 10\n",
    "\n",
    "# show the results\n",
    "results.sort_values(by=metric, ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise (optional)</strong>\n",
    "\n",
    "How do tokens influence the final result? Could we achieve better results with other type of tokens?\n",
    "\n",
    "Try to create tokens in a different way as explained in **Notebook 2.1.- Text normalization** (different tokenizer, with or without lemmatizing, stemming, stop words, case folding,...) and check the performance again.\n",
    "\n",
    "Does <kbd>gensim</kbd> provide also any tokenizer?\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color: #b1063a; color: #ffffff; padding: 10px;\">\n",
    "<strong>Exercise (optional)</strong>\n",
    "\n",
    "Would it be different if instead of using the average of the vectors in the descriptor we used the average fo the vectors weighted with TF-IDF?\n",
    "\n",
    "Write an implementation of TF-IDF for Word2Vec embeddings and try it with our task. How does it perform compared to the raw average?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>6. Advantages and disadvantages of Word2Vec Models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the pros and cons of Word2Vec embeddings, and see where can they be used.\n",
    "        \n",
    "#### Advantages:\n",
    "\n",
    "> **Semantic Relationships**: Word2Vec models capture relationships between words by representing them as dense vectors in a continuous vector space. While this representation allows for measuring global semantic similarities, it is more static compared to contextual embeddings. The semantic relationships in Word2Vec are derived from the overall usage patterns of words across the entire training corpus.\n",
    "> - **Word Similarity and Analogies**: Word2Vec models excel in capturing word similarities and analogies. By embedding words in a vector space, the model can perform operations such as vector arithmetic, allowing for analogical reasoning (e.g., king - man + woman = queen). This ability to represent and understand relationships between words is a significant strength.\n",
    ">  - **Contextual Information**: Word2Vec considers the context in which words appear, capturing not only the word itself but also the surrounding context. This contextual information contributes to a richer and more meaningful representation of words, addressing the limitations of BoW and TF-IDF models in terms of word order and semantics.\n",
    ">  - **Dimensionality Reduction**: Word2Vec models often result in lower-dimensional representations compared to high-dimensional BoW and TF-IDF vectors. This reduction in dimensionality contributes to computational efficiency and facilitates more effective use of the learned embeddings.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "> - **Lack of Interpretability**: While Word2Vec models provide powerful representations, the dense vector embeddings lack interpretability. It can be challenging to understand the specific factors contributing to the position of a word in the vector space, making it less transparent compared to BoW or TF-IDF models.\n",
    "> - **Dependency on Training Data Quality**: The quality of Word2Vec embeddings heavily depends on the training data. If the training corpus is not representative or lacks diversity, the model may produce biased or limited embeddings. Additionally, rare or out-of-vocabulary words may not be well-represented if not encountered frequently during training.\n",
    "> - **Fixed Vocabulary Size**: Word2Vec models typically have a fixed vocabulary size determined during training. This can be a limitation when dealing with evolving or dynamic vocabularies, as the model may struggle to adapt to new words introduced after training.\n",
    "> - **Difficulty Handling Polysemy**: Word2Vec models may struggle with polysemy, where a single word has multiple meanings. Since the model assigns a single vector to each word, it might not effectively capture the different senses of polysemous words.\n",
    "\n",
    "#### Applications:\n",
    "\n",
    "> - **Semantic Similarity and Clustering**: Word2Vec models find applications in measuring semantic similarity between words and clustering words with similar meanings. This is valuable in various natural language processing tasks, such as document similarity, information retrieval, and recommendation systems.\n",
    ">  - **Named Entity Recognition (NER)**: In Named Entity Recognition tasks, where the goal is to identify and classify entities (e.g., names of people, organizations) in text, Word2Vec embeddings can enhance the contextual understanding of words, improving the accuracy of entity recognition.\n",
    ">  - **Sentiment Analysis**: Word2Vec embeddings contribute to sentiment analysis tasks by capturing nuanced relationships between words and their emotional connotations. This enables more accurate sentiment classification in comparison to simpler models like BoW.\n",
    "> - **Language Translation**: Word2Vec embeddings have been utilized in machine translation tasks to capture cross-lingual semantic similarities, facilitating the translation of words and phrases based on their contextual representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
